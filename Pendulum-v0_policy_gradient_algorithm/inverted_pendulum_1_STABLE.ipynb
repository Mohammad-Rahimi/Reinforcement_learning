{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control of inverted pendulum with Reinforce algorithm\n",
    "\n",
    "\n",
    "### Policy Gradient\n",
    "\n",
    "In this code, I will implement vanilla policy gradient algorithm (REINFORCE) covered in the lecture. You will work on i) a function approximator, ii) computing action, iii) collecting samples, iV) training the agent, V) plotting the resutls. \n",
    "\n",
    "\n",
    "***Complete the missing operations and test your implemented algorithm on the Gym environment.***\n",
    "\n",
    "***Software requirements:***\n",
    "- Python >= 3.6\n",
    "- Tensorflow version <= 1.15.3 (1.X version)\n",
    "- OpenAI Gym\n",
    "\n",
    "- Training the agent (policy) can take long time. It is recomended to start solving the problems earlier.\n",
    "\n",
    "- Save any plots you generated in this notebook. The grade will be given based on the plots you showed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure the packages you installed meet the requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.13.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gym\n",
    "gym.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Tensorflow Implementation\n",
    "\n",
    "We will be implementing policy gradient algorithm using Tensorflow 1.X., which simply updates the parameters of policy from obtaining gradient estimates. The core of policy gradient is to design a function approximator, computing actions, collecting samples, and training the policy. In the below cell, you are encouraged to fill in the components that are missing. ***Your tasks*** are \n",
    "\n",
    "1. Complete the 'create_model' method to output the mean value for diagonal Guassian policy. Covariance is already defined in the model, so focus on creating neural network model.\n",
    "\n",
    "2. Complete the 'action_op' method to calculate and return the actions for diagonal Gaussian policy. The applied action should be $\\pi(s) = \\pi_{\\text{mean}}(s) + exp(logstd) * \\mathcal{N}(0,1)$\n",
    "\n",
    "***Hints***:\n",
    "- Some useful tensorflow classes and methods include: 'tf.exp', 'tf.random_normal'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "import numpy as np\n",
    "\n",
    "class PolicyOpt(object):\n",
    "    def __init__(self, env, linear=False, stochastic=True, hidden_size=32, nonlinearity=None):\n",
    "        \"\"\"\n",
    "        Minimal PyTorch port preserving the original algorithmic structure:\n",
    "        - Gaussian policy pi(a|s) with mean = network(s), diagonal std as a trainable parameter\n",
    "        - REINFORCE-compatible API surface (compute_action, log_likelihoods_eval, compute_expected_return)\n",
    "        \"\"\"\n",
    "        self.env = env\n",
    "        self.stochastic = stochastic\n",
    "        self.obs_dim = int(np.prod(env.observation_space.shape))\n",
    "        self.act_dim = int(np.prod(env.action_space.shape))\n",
    "        self.act_low  = torch.as_tensor(env.action_space.low,  dtype=torch.float32)\n",
    "        self.act_high = torch.as_tensor(env.action_space.high, dtype=torch.float32)\n",
    "\n",
    "        # Build policy network\n",
    "        layers = []\n",
    "        last = self.obs_dim\n",
    "        if linear:\n",
    "            layers.append(nn.Linear(last, self.act_dim))\n",
    "        else:\n",
    "            layers += [nn.Linear(last, hidden_size), nn.Tanh(), nn.Linear(hidden_size, self.act_dim)]\n",
    "        self.policy_net = nn.Sequential(*layers)\n",
    "\n",
    "        # Log-std as a free parameter (state-independent)\n",
    "        self.log_std = nn.Parameter(torch.full((self.act_dim,), -0.5))\n",
    "\n",
    "        # Optimizer placeholder (used in REINFORCE.train)\n",
    "        self.optimizer = None\n",
    "\n",
    "        # Default discount\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def _dist(self, obs_tensor: torch.Tensor) -> Normal:\n",
    "        mu = self.policy_net(obs_tensor)\n",
    "        std = torch.exp(self.log_std).expand_as(mu)\n",
    "        return Normal(mu, std)\n",
    "\n",
    "    def compute_action(self, s):\n",
    "        \"\"\"Return an action (np.ndarray) given a single state or batch of states.\"\"\"\n",
    "        s_t = torch.as_tensor(np.array(s, dtype=np.float32))\n",
    "        if s_t.ndim == 1:\n",
    "            s_t = s_t.unsqueeze(0)\n",
    "        dist = self._dist(s_t)\n",
    "        if self.stochastic:\n",
    "            a = dist.sample()\n",
    "        else:\n",
    "            a = dist.mean\n",
    "        a = a.clamp(self.act_low, self.act_high)\n",
    "        return a.detach().cpu().numpy()\n",
    "\n",
    "    def log_likelihoods_eval(self, states, actions):\n",
    "        \"\"\"Return log pi(a|s) for each (s,a) pair as a 1D numpy array.\"\"\"\n",
    "        s_t = torch.as_tensor(np.array(states, dtype=np.float32))\n",
    "        a_t = torch.as_tensor(np.array(actions, dtype=np.float32))\n",
    "        if s_t.ndim == 1: s_t = s_t.unsqueeze(0)\n",
    "        if a_t.ndim == 1: a_t = a_t.unsqueeze(0)\n",
    "        dist = self._dist(s_t)\n",
    "        logp = dist.log_prob(a_t)  # (batch, act_dim)\n",
    "        logp = logp.sum(dim=-1)    # sum over action dims\n",
    "        return logp.detach().cpu().numpy()\n",
    "\n",
    "    def compute_expected_return(self, samples, normalize=False):\n",
    "        \"\"\"\n",
    "        samples: list of dicts with key 'reward' -> list/array of rewards for each episode\n",
    "        returns: flattened reward-to-go across episodes (concatenated), optionally normalized\n",
    "        \"\"\"\n",
    "        all_G = []\n",
    "        for ep in samples:\n",
    "            r = np.asarray(ep[\"reward\"], dtype=np.float32)\n",
    "            G = np.zeros_like(r, dtype=np.float32)\n",
    "            g = 0.0\n",
    "            for t in range(len(r)-1, -1, -1):\n",
    "                g = r[t] + float(self.gamma) * g\n",
    "                G[t] = g\n",
    "            all_G.append(G)\n",
    "        out = np.concatenate(all_G, axis=0)\n",
    "        if normalize:\n",
    "            out = (out - out.mean()) / (out.std() + 1e-8)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Tensorflow Interpretation\n",
    "\n",
    "In order to test your implementation of the **stochastic policy**, run the below cell. The task is to interpret the code you implemented in previous section. If you implement correctly, you can see the value_1 and value_2.\n",
    "\n",
    "***Question: How do you interpret value_1 and value_2 below cell?***\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "policy mean (value_1): [[-0.3831538]]\n",
      "sampled/greedy action (value_2): [[0.29644418]]\n"
     ]
    }
   ],
   "source": [
    "TEST_ENV = gym.make(\"Pendulum-v0\")\n",
    "alg = PolicyOpt(TEST_ENV, linear=False)\n",
    "\n",
    "import numpy as np, torch\n",
    "input_1 = np.array([[0, 1, 2]], dtype=np.float32)\n",
    "\n",
    "# Put input on the same device as the model\n",
    "model_device = next(alg.policy_net.parameters()).device\n",
    "with torch.no_grad():\n",
    "    value_1 = alg.policy_net(\n",
    "        torch.as_tensor(input_1, dtype=torch.float32, device=model_device)\n",
    "    ).detach().cpu().numpy()\n",
    "\n",
    "value_2 = alg.compute_action(input_1)\n",
    "\n",
    "print(\"policy mean (value_1):\", value_1)\n",
    "print(\"sampled/greedy action (value_2):\", value_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.3831538]], dtype=float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.29644418]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "value_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Implement Policy Gradient\n",
    "\n",
    "In this section, we will implement REINFORCE algorithm presented in the lecture. As a review, the objective is optimize the parameters $\\theta$ of some policy $\\pi_\\theta$ so that the expected return\n",
    "\n",
    "\\begin{equation}\n",
    "J(\\theta) = \\mathbb{E} \\bigg\\{ \\sum_{t=0}^T \\gamma^t r(s_{t},a_{t}) \\bigg\\}\n",
    "\\end{equation}\n",
    "\n",
    "is optimized. In this algorithm, this is done by calculating the gradient $\\nabla_\\theta J$ and applying a gradient descent method to find a better policy.\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta ' = \\theta + \\alpha \\nabla_\\theta J(\\theta)\n",
    "\\end{equation}\n",
    "\n",
    "In the lecture, we derive how we compute $\\nabla_{\\theta} J(\\theta)$. We can rewrite our policy gradient as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J (\\theta) \\approx \\frac{1}{N} \\sum_{i=0}^{N} \\bigg( \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta (a_{it} | s_{it}) \\bigg) \\bigg( \\sum_{t=0}^T \\gamma^{t}r_i(t) \\bigg)\n",
    "\\end{equation}\n",
    "\n",
    "Finally, taking into account the causality principle discussed in class, we are able to simplifiy the gradient estimate such as:\n",
    "\n",
    "\\begin{equation}\n",
    "\\nabla_\\theta J (\\theta) \\approx \\frac{1}{N} \\sum_{i=0}^{N} \\sum_{t=0}^{T} \\nabla_\\theta \\log \\pi_\\theta (a_{it} | s_{it}) \\sum_{t'=t}^T \\gamma^{t'-t}r_i(t')\n",
    "\\end{equation}\n",
    "\n",
    "You will be implementing final expression in this assignment!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process of REINFOCE algorithm follows:\n",
    "\n",
    "1. Collect samples from current policy $\\pi_\\theta(s)$ by executing rollouts of the environment.\n",
    "2. Calculate an estimate for the expected return at state $s_t$. \n",
    "3. Compute the log-likelihood of each action that was performed by the policy at every given step.\n",
    "4. Estimate the gradient and update the parameters of policy using gradient-based technique.\n",
    "5. Repeat steps 1-4 for a number of training iterations.\n",
    "\n",
    "***Your task*** is to fill out the skeleton code for REINFORCE algorithm,\n",
    "\n",
    "1. Complete the 'log_likelihoods' method to compute gradient of policy, $\\nabla_{\\theta}\\pi_{\\theta}$ for diagonal Guassian policy. \n",
    "\n",
    "2. Complete the 'compute_expected_return' method to calculate the reward-to-go, $\\sum_{t^{\\prime}=t}^{T}$. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class REINFORCE(PolicyOpt):\n",
    "    def train(self, num_iterations=1000, steps_per_iteration=1000, learning_rate=3e-5, gamma=0.95, **kwargs):\n",
    "        \"\"\"\n",
    "        Vanilla REINFORCE (Monte-Carlo policy gradient) with a Gaussian policy on continuous actions.\n",
    "        No baseline, no entropy bonus, no normalization.\n",
    "        \"\"\"\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(list(self.policy_net.parameters()) + [self.log_std], lr=float(learning_rate))\n",
    "\n",
    "        episode_returns = []\n",
    "        env = self.env\n",
    "\n",
    "        total_steps = 0\n",
    "        for it in range(num_iterations):\n",
    "            \n",
    "            # clamp exploration scale each iteration to avoid σ->0 or exploding σ\n",
    "            with torch.no_grad():\n",
    "                self.log_std.data.clamp_(-2.0, 1.0)\n",
    "\n",
    "            # Collect trajectories until reaching steps_per_iteration\n",
    "            batch_states = []\n",
    "            batch_actions = []\n",
    "            batch_returns = []\n",
    "            steps_in_batch = 0\n",
    "\n",
    "            while steps_in_batch < steps_per_iteration:\n",
    "                # start new episode\n",
    "                out = env.reset()\n",
    "                obs = out[0] if isinstance(out, tuple) else out\n",
    "                ep_states, ep_actions, ep_rewards = [], [], []\n",
    "                done = False\n",
    "                while not done:\n",
    "                    s_t = torch.as_tensor(obs, dtype=torch.float32).unsqueeze(0)\n",
    "                    dist = self._dist(s_t)\n",
    "                    a_t = dist.sample() if self.stochastic else dist.mean\n",
    "                    a_np = a_t.squeeze(0).detach().cpu().numpy()\n",
    "                    a_np = np.clip(a_np, self.act_low.numpy(), self.act_high.numpy())\n",
    "                    step_out = env.step(a_np)\n",
    "                    if len(step_out) == 5:\n",
    "                        obs_next, r, terminated, truncated, info = step_out\n",
    "                        done = bool(terminated or truncated)\n",
    "                    else:\n",
    "                        obs_next, r, done, info = step_out\n",
    "                        done = bool(done)\n",
    "\n",
    "                    ep_states.append(obs)\n",
    "                    ep_actions.append(a_np)\n",
    "                    ep_rewards.append(float(r))\n",
    "\n",
    "                    obs = obs_next\n",
    "                    steps_in_batch += 1\n",
    "                    if done:\n",
    "                        # compute reward-to-go for this episode and log return\n",
    "                        G = np.zeros_like(ep_rewards, dtype=np.float32)\n",
    "                        g = 0.0\n",
    "                        for t in range(len(ep_rewards)-1, -1, -1):\n",
    "                            g = ep_rewards[t] + gamma * g\n",
    "                            G[t] = g\n",
    "                        batch_states.extend(ep_states)\n",
    "                        batch_actions.extend(ep_actions)\n",
    "                        batch_returns.extend(G.tolist())\n",
    "                        episode_returns.append(sum(ep_rewards))\n",
    "                        break\n",
    "\n",
    "            # Convert to tensors\n",
    "            s_b = torch.as_tensor(np.array(batch_states, dtype=np.float32))\n",
    "            a_b = torch.as_tensor(np.array(batch_actions, dtype=np.float32))\n",
    "            G_b = torch.as_tensor(np.array(batch_returns, dtype=np.float32))\n",
    "\n",
    "            # Policy loss = -E[ log pi(a|s) * G ]\n",
    "            dist_b = self._dist(s_b)\n",
    "            logp = dist_b.log_prob(a_b).sum(dim=-1)\n",
    "            loss = -(logp * G_b).mean()\n",
    "\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            # clip gradients to avoid rare spikes\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_steps += steps_in_batch\n",
    "            if (it+1) % max(1, num_iterations//10) == 0:\n",
    "                print(f\"[Iter {it+1}/{num_iterations}] Steps: {total_steps} | Loss: {float(loss.item()):.4f} | EpRet(last): {episode_returns[-1]:.1f}\")\n",
    "\n",
    "        return episode_returns\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check your 'log_likelihoods' method by running below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihoods: [ -0.4189842  -3.232242  -10.284169 ]\n"
     ]
    }
   ],
   "source": [
    "alg = REINFORCE(TEST_ENV, stochastic=True)\n",
    "pass  # PyTorch version: call log_likelihoods_eval directly\n",
    "# collect a sample output for a given input state\n",
    "input_s = [[0, 0, 0], [0, 1, 2], [1, 2, 3]]\n",
    "input_a = [[0], [1], [2]]\n",
    "\n",
    "# Check\n",
    "computed = alg.log_likelihoods_eval(states=input_s, actions=input_a)\n",
    "\n",
    "print('log_likelihoods:', computed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test your 'compute_expected_return' by running below cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Great job!\n"
     ]
    }
   ],
   "source": [
    "# 1. Test the non-normalized case\n",
    "alg = REINFORCE(TEST_ENV, stochastic=True)\n",
    "alg.gamma = 1.0\n",
    "    \n",
    "input_1 = [{\"reward\": [1, 1, 1, 1]},\n",
    "           {\"reward\": [1, 1, 1, 1]}]\n",
    "vs_1 = alg.compute_expected_return(samples=input_1)\n",
    "ans_1 = np.array([4, 3, 2, 1, 4, 3, 2, 1])\n",
    "\n",
    "if np.linalg.norm(vs_1 - ans_1) < 1e-3:\n",
    "    print('Great job!')\n",
    "else:\n",
    "    print('Check your implementation (compute_expected_return)')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Testing your algorithm\n",
    "\n",
    "When you are ready, test your policy gradient algorithms on the *Pendulum-v0* environment in the cell below. *Pendulum-v0* environment is similar to *off-shore wind power*, the goal here is to maintain the Pendulum is upright using control input. The best policy should get around -200 scores. ***Your task*** is to run your REINFORCE algorithm and plot the result!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==== Training Run 0 ====\n",
      "[Iter 50/500] Steps: 250000 | Loss: -532.3245 | EpRet(last): -1471.3\n",
      "[Iter 100/500] Steps: 500000 | Loss: -26187.5176 | EpRet(last): -1497.6\n",
      "[Iter 150/500] Steps: 750000 | Loss: -234914.4375 | EpRet(last): -1214.3\n",
      "[Iter 200/500] Steps: 1000000 | Loss: -2514733.2500 | EpRet(last): -1215.1\n",
      "[Iter 250/500] Steps: 1250000 | Loss: -4825278.0000 | EpRet(last): -1656.1\n",
      "[Iter 300/500] Steps: 1500000 | Loss: -7234583.0000 | EpRet(last): -1201.2\n",
      "[Iter 350/500] Steps: 1750000 | Loss: -11491135.0000 | EpRet(last): -1491.4\n",
      "[Iter 400/500] Steps: 2000000 | Loss: -14423032.0000 | EpRet(last): -1169.5\n",
      "[Iter 450/500] Steps: 2250000 | Loss: -18495330.0000 | EpRet(last): -1319.2\n",
      "[Iter 500/500] Steps: 2500000 | Loss: -24073502.0000 | EpRet(last): -1346.3\n",
      "\n",
      "==== Training Run 1 ====\n",
      "[Iter 50/500] Steps: 250000 | Loss: -111.8563 | EpRet(last): -1127.3\n",
      "[Iter 100/500] Steps: 500000 | Loss: -130.7528 | EpRet(last): -1178.9\n",
      "[Iter 150/500] Steps: 750000 | Loss: -3266.5950 | EpRet(last): -1544.6\n",
      "[Iter 200/500] Steps: 1000000 | Loss: -84725.4375 | EpRet(last): -1609.8\n",
      "[Iter 250/500] Steps: 1250000 | Loss: -951623.6875 | EpRet(last): -1491.1\n",
      "[Iter 300/500] Steps: 1500000 | Loss: -3780291.7500 | EpRet(last): -1357.0\n",
      "[Iter 350/500] Steps: 1750000 | Loss: -6184368.0000 | EpRet(last): -1594.1\n",
      "[Iter 400/500] Steps: 2000000 | Loss: -8982647.0000 | EpRet(last): -1653.1\n",
      "[Iter 450/500] Steps: 2250000 | Loss: -12389056.0000 | EpRet(last): -1655.3\n",
      "[Iter 500/500] Steps: 2500000 | Loss: -16059636.0000 | EpRet(last): -1613.0\n",
      "\n",
      "==== Training Run 2 ====\n",
      "[Iter 50/500] Steps: 250000 | Loss: -373.4641 | EpRet(last): -1439.8\n",
      "[Iter 100/500] Steps: 500000 | Loss: -24110.2227 | EpRet(last): -1502.3\n",
      "[Iter 150/500] Steps: 750000 | Loss: -261609.1094 | EpRet(last): -1123.0\n",
      "[Iter 200/500] Steps: 1000000 | Loss: -1699346.3750 | EpRet(last): -1255.9\n",
      "[Iter 250/500] Steps: 1250000 | Loss: -4821586.0000 | EpRet(last): -1172.6\n",
      "[Iter 300/500] Steps: 1500000 | Loss: -7094899.5000 | EpRet(last): -1640.2\n",
      "[Iter 350/500] Steps: 1750000 | Loss: -10216925.0000 | EpRet(last): -1291.2\n",
      "[Iter 400/500] Steps: 2000000 | Loss: -13309125.0000 | EpRet(last): -1501.9\n",
      "[Iter 450/500] Steps: 2250000 | Loss: -19135924.0000 | EpRet(last): -1183.8\n",
      "[Iter 500/500] Steps: 2500000 | Loss: -22061804.0000 | EpRet(last): -1068.2\n"
     ]
    }
   ],
   "source": [
    "# set this number as 1 for testing your algorithm, and 3 for plotting\n",
    "NUM_TRIALS = 3\n",
    "\n",
    "# ===========================================================================\n",
    "# Do not modify below line\n",
    "# ===========================================================================\n",
    "\n",
    "# we will test the algorithms on the Pendulum-v1 gym environment\n",
    "\n",
    "env = gym.make(\"Pendulum-v0\")\n",
    "\n",
    "# train on the REINFORCE algorithm\n",
    "import numpy as np\n",
    "r = []\n",
    "for i in range(NUM_TRIALS):\n",
    "    print(\"\\n==== Training Run {} ====\".format(i))\n",
    "    alg = REINFORCE(env, stochastic=True)\n",
    "    res = alg.train(learning_rate=0.005, gamma=0.95, num_iterations=500, steps_per_iteration=5000)\n",
    "    r.append(np.array(res))\n",
    "    alg = None\n",
    "\n",
    "# save results\n",
    "np.savetxt(\"InvertedPendulum_results.csv\", np.array(r), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "IPython -- An enhanced Interactive Python\n",
       "=========================================\n",
       "\n",
       "IPython offers a fully compatible replacement for the standard Python\n",
       "interpreter, with convenient shell features, special commands, command\n",
       "history mechanism and output results caching.\n",
       "\n",
       "At your system command line, type 'ipython -h' to see the command line\n",
       "options available. This document only describes interactive features.\n",
       "\n",
       "GETTING HELP\n",
       "------------\n",
       "\n",
       "Within IPython you have various way to access help:\n",
       "\n",
       "  ?         -> Introduction and overview of IPython's features (this screen).\n",
       "  object?   -> Details about 'object'.\n",
       "  object??  -> More detailed, verbose information about 'object'.\n",
       "  %quickref -> Quick reference of all IPython specific syntax and magics.\n",
       "  help      -> Access Python's own help system.\n",
       "\n",
       "If you are in terminal IPython you can quit this screen by pressing `q`.\n",
       "\n",
       "\n",
       "MAIN FEATURES\n",
       "-------------\n",
       "\n",
       "* Access to the standard Python help with object docstrings and the Python\n",
       "  manuals. Simply type 'help' (no quotes) to invoke it.\n",
       "\n",
       "* Magic commands: type %magic for information on the magic subsystem.\n",
       "\n",
       "* System command aliases, via the %alias command or the configuration file(s).\n",
       "\n",
       "* Dynamic object information:\n",
       "\n",
       "  Typing ?word or word? prints detailed information about an object. Certain\n",
       "  long strings (code, etc.) get snipped in the center for brevity.\n",
       "\n",
       "  Typing ??word or word?? gives access to the full information without\n",
       "  snipping long strings. Strings that are longer than the screen are printed\n",
       "  through the less pager.\n",
       "\n",
       "  The ?/?? system gives access to the full source code for any object (if\n",
       "  available), shows function prototypes and other useful information.\n",
       "\n",
       "  If you just want to see an object's docstring, type '%pdoc object' (without\n",
       "  quotes, and without % if you have automagic on).\n",
       "\n",
       "* Tab completion in the local namespace:\n",
       "\n",
       "  At any time, hitting tab will complete any available python commands or\n",
       "  variable names, and show you a list of the possible completions if there's\n",
       "  no unambiguous one. It will also complete filenames in the current directory.\n",
       "\n",
       "* Search previous command history in multiple ways:\n",
       "\n",
       "  - Start typing, and then use arrow keys up/down or (Ctrl-p/Ctrl-n) to search\n",
       "    through the history items that match what you've typed so far.\n",
       "\n",
       "  - Hit Ctrl-r: opens a search prompt. Begin typing and the system searches\n",
       "    your history for lines that match what you've typed so far, completing as\n",
       "    much as it can.\n",
       "\n",
       "  - %hist: search history by index.\n",
       "\n",
       "* Persistent command history across sessions.\n",
       "\n",
       "* Logging of input with the ability to save and restore a working session.\n",
       "\n",
       "* System shell with !. Typing !ls will run 'ls' in the current directory.\n",
       "\n",
       "* The reload command does a 'deep' reload of a module: changes made to the\n",
       "  module since you imported will actually be available without having to exit.\n",
       "\n",
       "* Verbose and colored exception traceback printouts. See the magic xmode and\n",
       "  xcolor functions for details (just type %magic).\n",
       "\n",
       "* Input caching system:\n",
       "\n",
       "  IPython offers numbered prompts (In/Out) with input and output caching. All\n",
       "  input is saved and can be retrieved as variables (besides the usual arrow\n",
       "  key recall).\n",
       "\n",
       "  The following GLOBAL variables always exist (so don't overwrite them!):\n",
       "  _i: stores previous input.\n",
       "  _ii: next previous.\n",
       "  _iii: next-next previous.\n",
       "  _ih : a list of all input _ih[n] is the input from line n.\n",
       "\n",
       "  Additionally, global variables named _i<n> are dynamically created (<n>\n",
       "  being the prompt counter), such that _i<n> == _ih[<n>]\n",
       "\n",
       "  For example, what you typed at prompt 14 is available as _i14 and _ih[14].\n",
       "\n",
       "  You can create macros which contain multiple input lines from this history,\n",
       "  for later re-execution, with the %macro function.\n",
       "\n",
       "  The history function %hist allows you to see any part of your input history\n",
       "  by printing a range of the _i variables. Note that inputs which contain\n",
       "  magic functions (%) appear in the history with a prepended comment. This is\n",
       "  because they aren't really valid Python code, so you can't exec them.\n",
       "\n",
       "* Output caching system:\n",
       "\n",
       "  For output that is returned from actions, a system similar to the input\n",
       "  cache exists but using _ instead of _i. Only actions that produce a result\n",
       "  (NOT assignments, for example) are cached. If you are familiar with\n",
       "  Mathematica, IPython's _ variables behave exactly like Mathematica's %\n",
       "  variables.\n",
       "\n",
       "  The following GLOBAL variables always exist (so don't overwrite them!):\n",
       "  _ (one underscore): previous output.\n",
       "  __ (two underscores): next previous.\n",
       "  ___ (three underscores): next-next previous.\n",
       "\n",
       "  Global variables named _<n> are dynamically created (<n> being the prompt\n",
       "  counter), such that the result of output <n> is always available as _<n>.\n",
       "\n",
       "  Finally, a global dictionary named _oh exists with entries for all lines\n",
       "  which generated output.\n",
       "\n",
       "* Directory history:\n",
       "\n",
       "  Your history of visited directories is kept in the global list _dh, and the\n",
       "  magic %cd command can be used to go to any entry in that list.\n",
       "\n",
       "* Auto-parentheses and auto-quotes (adapted from Nathan Gray's LazyPython)\n",
       "\n",
       "  1. Auto-parentheses\n",
       "        \n",
       "     Callable objects (i.e. functions, methods, etc) can be invoked like\n",
       "     this (notice the commas between the arguments)::\n",
       "       \n",
       "         In [1]: callable_ob arg1, arg2, arg3\n",
       "       \n",
       "     and the input will be translated to this::\n",
       "       \n",
       "         callable_ob(arg1, arg2, arg3)\n",
       "       \n",
       "     This feature is off by default (in rare cases it can produce\n",
       "     undesirable side-effects), but you can activate it at the command-line\n",
       "     by starting IPython with `--autocall 1`, set it permanently in your\n",
       "     configuration file, or turn on at runtime with `%autocall 1`.\n",
       "\n",
       "     You can force auto-parentheses by using '/' as the first character\n",
       "     of a line.  For example::\n",
       "       \n",
       "          In [1]: /globals             # becomes 'globals()'\n",
       "       \n",
       "     Note that the '/' MUST be the first character on the line!  This\n",
       "     won't work::\n",
       "       \n",
       "          In [2]: print /globals    # syntax error\n",
       "\n",
       "     In most cases the automatic algorithm should work, so you should\n",
       "     rarely need to explicitly invoke /. One notable exception is if you\n",
       "     are trying to call a function with a list of tuples as arguments (the\n",
       "     parenthesis will confuse IPython)::\n",
       "       \n",
       "          In [1]: zip (1,2,3),(4,5,6)  # won't work\n",
       "       \n",
       "     but this will work::\n",
       "       \n",
       "          In [2]: /zip (1,2,3),(4,5,6)\n",
       "          ------> zip ((1,2,3),(4,5,6))\n",
       "          Out[2]= [(1, 4), (2, 5), (3, 6)]\n",
       "\n",
       "     IPython tells you that it has altered your command line by\n",
       "     displaying the new command line preceded by -->.  e.g.::\n",
       "       \n",
       "          In [18]: callable list\n",
       "          -------> callable (list)\n",
       "\n",
       "  2. Auto-Quoting\n",
       "    \n",
       "     You can force auto-quoting of a function's arguments by using ',' as\n",
       "     the first character of a line.  For example::\n",
       "       \n",
       "          In [1]: ,my_function /home/me   # becomes my_function(\"/home/me\")\n",
       "\n",
       "     If you use ';' instead, the whole argument is quoted as a single\n",
       "     string (while ',' splits on whitespace)::\n",
       "       \n",
       "          In [2]: ,my_function a b c   # becomes my_function(\"a\",\"b\",\"c\")\n",
       "          In [3]: ;my_function a b c   # becomes my_function(\"a b c\")\n",
       "\n",
       "     Note that the ',' MUST be the first character on the line!  This\n",
       "     won't work::\n",
       "       \n",
       "          In [4]: x = ,my_function /home/me    # syntax error\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# collect saved results\n",
    "import numpy as np\n",
    "r1 = np.genfromtxt(\"InvertedPendulum_results.csv\", delimiter=\",\")\n",
    "all_results = [r1]\n",
    "labels = [\"REINFORCE\"]\n",
    "\n",
    "##############################################################\n",
    "# Plot your Policy Gradient results below\n",
    "##############################################################\n",
    "?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- PolicyOpt (PyTorch) + REINFORCE (vanilla), CUDA-ready ---\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Normal\n",
    "\n",
    "# Device\n",
    "try:\n",
    "    DEVICE\n",
    "except NameError:\n",
    "    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "class PolicyOpt(object):\n",
    "    def __init__(self, env, linear=False, stochastic=True, hidden_size=32, nonlinearity=None):\n",
    "        \"\"\"Gaussian policy π(a|s): mean = MLP(s), diag std = exp(log_std).\"\"\"\n",
    "        self.env = env\n",
    "        self.stochastic = stochastic\n",
    "        self.obs_dim = int(np.prod(env.observation_space.shape))\n",
    "        self.act_dim = int(np.prod(env.action_space.shape))\n",
    "\n",
    "        # Action bounds (on device) + cached NumPy copies\n",
    "        self.act_low  = torch.as_tensor(env.action_space.low,  dtype=torch.float32, device=DEVICE)\n",
    "        self.act_high = torch.as_tensor(env.action_space.high, dtype=torch.float32, device=DEVICE)\n",
    "        self.act_low_np  = self.act_low.detach().cpu().numpy()\n",
    "        self.act_high_np = self.act_high.detach().cpu().numpy()\n",
    "\n",
    "        # Policy network\n",
    "        layers, last = [], self.obs_dim\n",
    "        if linear:\n",
    "            layers.append(nn.Linear(last, self.act_dim))\n",
    "        else:\n",
    "            layers += [nn.Linear(last, hidden_size), nn.Tanh(), nn.Linear(hidden_size, self.act_dim)]\n",
    "        self.policy_net = nn.Sequential(*layers).to(DEVICE)\n",
    "\n",
    "        # State-independent log std (trainable)\n",
    "        self.log_std = nn.Parameter(torch.full((self.act_dim,), -0.5, device=DEVICE))\n",
    "\n",
    "        self.optimizer = None\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def _dist(self, obs_tensor: torch.Tensor) -> Normal:\n",
    "        mu = self.policy_net(obs_tensor)\n",
    "        std = torch.exp(self.log_std).expand_as(mu)\n",
    "        return Normal(mu, std)\n",
    "\n",
    "    def compute_action(self, s):\n",
    "        \"\"\"Return an action for a single state or batch of states.\"\"\"\n",
    "        s_t = torch.as_tensor(np.array(s, dtype=np.float32), device=DEVICE)\n",
    "        if s_t.ndim == 1:\n",
    "            s_t = s_t.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            dist = self._dist(s_t)\n",
    "            a = dist.sample() if self.stochastic else dist.mean\n",
    "        a = a.clamp(self.act_low, self.act_high)\n",
    "        return a.detach().cpu().numpy()\n",
    "\n",
    "    def log_likelihoods_eval(self, states, actions):\n",
    "        \"\"\"Return log π(a|s) for each (s,a) pair: shape [T].\"\"\"\n",
    "        s_t = torch.as_tensor(np.array(states, dtype=np.float32), device=DEVICE)\n",
    "        a_t = torch.as_tensor(np.array(actions, dtype=np.float32), device=DEVICE)\n",
    "        if s_t.ndim == 1: s_t = s_t.unsqueeze(0)\n",
    "        if a_t.ndim == 1: a_t = a_t.unsqueeze(0)\n",
    "        dist = self._dist(s_t)\n",
    "        logp = dist.log_prob(a_t).sum(dim=-1)\n",
    "        return logp.detach().cpu().numpy()\n",
    "\n",
    "    def compute_expected_return(self, samples, normalize=False):\n",
    "        \"\"\"Concatenate reward-to-go across all episodes.\"\"\"\n",
    "        all_G = []\n",
    "        for ep in samples:\n",
    "            r = np.asarray(ep[\"reward\"], dtype=np.float32)\n",
    "            G = np.zeros_like(r, dtype=np.float32)\n",
    "            g = 0.0\n",
    "            for t in range(len(r) - 1, -1, -1):\n",
    "                g = r[t] + float(self.gamma) * g\n",
    "                G[t] = g\n",
    "            all_G.append(G)\n",
    "        out = np.concatenate(all_G, axis=0)\n",
    "        if normalize and out.size > 1:\n",
    "            out = (out - out.mean()) / (out.std() + 1e-8)\n",
    "        return out\n",
    "\n",
    "\n",
    "class REINFORCE(PolicyOpt):\n",
    "    def train(\n",
    "        self,\n",
    "        num_iterations: int = 1000,\n",
    "        steps_per_iteration: int = 1000,\n",
    "        learning_rate: float = 3e-5,   # keep float; int(1e-4) == 0!\n",
    "        gamma: float = 0.95,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"Vanilla REINFORCE (no baseline/entropy/normalization).\"\"\"\n",
    "        import time\n",
    "        self.gamma = gamma\n",
    "        self.optimizer = optim.Adam(\n",
    "            list(self.policy_net.parameters()) + [self.log_std],\n",
    "            lr=float(learning_rate),\n",
    "        )\n",
    "\n",
    "        episode_returns = []\n",
    "        env = self.env\n",
    "        total_steps = 0\n",
    "\n",
    "        for it in range(num_iterations):\n",
    "            \n",
    "            # clamp exploration scale each iteration to avoid σ->0 or exploding σ\n",
    "            with torch.no_grad():\n",
    "                self.log_std.data.clamp_(-2.0, 1.0)\n",
    "\n",
    "            t0 = time.perf_counter()\n",
    "            batch_states, batch_actions, batch_returns = [], [], []\n",
    "            steps_in_batch = 0\n",
    "\n",
    "            while steps_in_batch < steps_per_iteration:\n",
    "                out = env.reset()\n",
    "                obs = out[0] if isinstance(out, tuple) else out\n",
    "\n",
    "                ep_states, ep_actions, ep_rewards = [], [], []\n",
    "                done = False\n",
    "\n",
    "                low_np, high_np = self.act_low_np, self.act_high_np\n",
    "\n",
    "                while not done:\n",
    "                    s_t = torch.from_numpy(np.asarray(obs, np.float32)).to(DEVICE).unsqueeze(0)\n",
    "                    with torch.no_grad():\n",
    "                        dist = self._dist(s_t)\n",
    "                        a_t = dist.sample() if self.stochastic else dist.mean\n",
    "\n",
    "                    a_np = a_t.squeeze(0).detach().cpu().numpy()\n",
    "                    a_np = np.clip(a_np, low_np, high_np)\n",
    "\n",
    "                    step_out = env.step(a_np)\n",
    "                    if len(step_out) == 5:\n",
    "                        obs_next, r, terminated, truncated, info = step_out\n",
    "                        done = bool(terminated or truncated)\n",
    "                    else:\n",
    "                        obs_next, r, done, info = step_out\n",
    "                        done = bool(done)\n",
    "\n",
    "                    ep_states.append(obs)\n",
    "                    ep_actions.append(a_np)\n",
    "                    ep_rewards.append(float(r))\n",
    "\n",
    "                    obs = obs_next\n",
    "                    steps_in_batch += 1\n",
    "\n",
    "                    if done:\n",
    "                        G = np.zeros_like(ep_rewards, dtype=np.float32)\n",
    "                        g = 0.0\n",
    "                        for t in range(len(ep_rewards) - 1, -1, -1):\n",
    "                            g = ep_rewards[t] + gamma * g\n",
    "                            G[t] = g\n",
    "\n",
    "                        batch_states.extend(ep_states)\n",
    "                        batch_actions.extend(ep_actions)\n",
    "                        batch_returns.extend(G.tolist())\n",
    "                        episode_returns.append(sum(ep_rewards))\n",
    "                        break\n",
    "\n",
    "            # Gradient step on the batch\n",
    "            s_b = torch.as_tensor(np.array(batch_states,  dtype=np.float32), device=DEVICE)\n",
    "            a_b = torch.as_tensor(np.array(batch_actions, dtype=np.float32), device=DEVICE)\n",
    "            G_b = torch.as_tensor(np.array(batch_returns, dtype=np.float32), device=DEVICE)\n",
    "\n",
    "            dist_b = self._dist(s_b)\n",
    "            logp   = dist_b.log_prob(a_b).sum(dim=-1)\n",
    "            loss   = -(logp * G_b).mean()\n",
    "\n",
    "            self.optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            # clip gradients to avoid rare spikes\n",
    "            torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), 1.0)\n",
    "            self.optimizer.step()\n",
    "\n",
    "            total_steps += steps_in_batch\n",
    "            dt = time.perf_counter() - t0\n",
    "            if (it + 1) % max(1, num_iterations // 10) == 0:\n",
    "                print(f\"[Iter {it+1}/{num_iterations}] \"\n",
    "                      f\"Steps: {total_steps} (+{steps_in_batch} @ {steps_in_batch/max(dt,1e-6):.0f} steps/s) | \"\n",
    "                      f\"Loss: {float(loss.item()):.4f} | EpRet(last): {episode_returns[-1]:.1f}\")\n",
    "        return episode_returns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
